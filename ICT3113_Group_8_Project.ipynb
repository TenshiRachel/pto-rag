{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p data"
      ],
      "metadata": {
        "id": "x9jPM-E-u9fi"
      },
      "id": "x9jPM-E-u9fi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qpSqtGHpoi_",
        "outputId": "9759663e-8541-441b-8974-36013d652913"
      },
      "id": "6qpSqtGHpoi_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb8e4733",
      "metadata": {
        "id": "bb8e4733"
      },
      "source": [
        "# Agent CFO — Performance Optimization & Design\n",
        "\n",
        "---\n",
        "This is Group 8's notebook for the ICT3113 Performance Testing and Optimization project following the required structure.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will design and optimize an Agent CFO assistant for a listed company. The assistant should answer finance/operations questions using RAG (Retrieval-Augmented Generation) + agentic reasoning, with response time (latency) as the primary metric.\n",
        "\n",
        "Your system must:\n",
        "*   Ingest the company’s public filings.\n",
        "*   Retrieve relevant passages efficiently.\n",
        "*   Compute ratios/trends via tool calls (calculator, table parsing).\n",
        "*   Produce answers with valid citations to the correct page/table.\n"
      ],
      "metadata": {
        "id": "wkMIj4Ssetku"
      },
      "id": "wkMIj4Ssetku"
    },
    {
      "cell_type": "markdown",
      "id": "0c138dd7",
      "metadata": {
        "id": "0c138dd7"
      },
      "source": [
        "## 1. Config & Secrets\n",
        "\n",
        "Fill in your API keys in secrets. **Do not hardcode keys** in cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a6098a4",
      "metadata": {
        "id": "8a6098a4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('PTO_KEY')\n",
        "\n",
        "COMPANY_NAME = \"NVIDIA\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b7a81e9",
      "metadata": {
        "id": "8b7a81e9"
      },
      "source": [
        "## 2. Data Download (Dropbox)\n",
        "\n",
        "*   Annual Reports: last 3–5 years.\n",
        "*   Quarterly Results Packs & MD&A (Management Discussion & Analysis).\n",
        "*   Investor Presentations and Press Releases.\n",
        "*   These files must be submitted later as a deliverable in the Dropbox data pack.\n",
        "*   Upload them under `/content/data/`.\n",
        "\n",
        "Scope limit: each team will ingest minimally 15 PDF files total.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0d4e754",
      "metadata": {
        "id": "b0d4e754"
      },
      "source": [
        "## 3. System Requirements\n",
        "\n",
        "**Retrieval & RAG**\n",
        "*   Use a vector index (e.g., FAISS, LlamaIndex) + a keyword filter (BM25/ElasticSearch).\n",
        "*   Citations must include: report name, year, page number, section/table.\n",
        "\n",
        "**Agentic Reasoning**\n",
        "*   Support at least 3 tool types: calculator, table extraction, multi-document compare.\n",
        "*   Reasoning must follow a plan-then-act pattern (not a single unstructured call).\n",
        "\n",
        "**Instrumentation**\n",
        "*   Log timings for: T_ingest, T_retrieve, T_rerank, T_reason, T_generate, T_total.\n",
        "*   Log: tokens used, cache hits, tools invoked.\n",
        "*   Record p50/p95 latencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e08f5a0",
      "metadata": {
        "id": "5e08f5a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a862746-9fc9-4d33-8aeb-024b2a4345c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.31-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.35-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting rank-bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-6.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langchain-core<2.0.0,>=0.3.78 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.3.78)\n",
            "Requirement already satisfied: langchain<2.0.0,>=0.3.27 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.43)\n",
            "Collecting requests<3.0.0,>=2.32.5 (from langchain-community)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.11.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.33)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.2)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2024.11.6)\n",
            "Collecting pdfminer.six==20250506 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (11.3.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.3)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain-community) (0.3.11)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.78->langchain-community) (1.33)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.2.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.0.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=0.3.78->langchain-community) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.23)\n",
            "Downloading langchain_community-0.3.31-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.35-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Downloading pypdf-6.1.1-py3-none-any.whl (323 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.5/323.5 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m118.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: requests, rank-bm25, pypdfium2, pypdf, mypy-extensions, marshmallow, faiss-cpu, typing-inspect, pdfminer.six, dataclasses-json, pdfplumber, langchain-openai, langchain-community\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 faiss-cpu-1.12.0 langchain-community-0.3.31 langchain-openai-0.3.35 marshmallow-3.26.1 mypy-extensions-1.1.0 pdfminer.six-20250506 pdfplumber-0.11.7 pypdf-6.1.1 pypdfium2-4.30.0 rank-bm25-0.2.2 requests-2.32.5 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies for ingestion pipeline\n",
        "!pip install langchain-community langchain-openai faiss-cpu rank-bm25 openai tiktoken pypdf pdfplumber"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyinstrument"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjT6Q12Sdr3t",
        "outputId": "c12fd35c-e068-4173-b2ae-5e7d348efa77"
      },
      "id": "rjT6Q12Sdr3t",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyinstrument\n",
            "  Downloading pyinstrument-5.1.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (26 kB)\n",
            "Downloading pyinstrument-5.1.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (146 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/146.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.8/146.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyinstrument\n",
            "Successfully installed pyinstrument-5.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import libraries for ingestion pipeline"
      ],
      "metadata": {
        "id": "WU1IQ5-Ta4ks"
      },
      "id": "WU1IQ5-Ta4ks"
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import glob\n",
        "import json\n",
        "import re\n",
        "import pdfplumber\n",
        "from pypdf import PdfReader\n",
        "from collections import defaultdict\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "from langchain.schema import Document\n",
        "from langchain.retrievers import SVMRetriever\n",
        "from langchain_core.retrievers import BaseRetriever\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain.tools import StructuredTool\n",
        "from langchain.agents import initialize_agent, AgentType\n",
        "from typing import List, Dict, Optional"
      ],
      "metadata": {
        "id": "KOXp3fSTa22D"
      },
      "id": "KOXp3fSTa22D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Go Gdrive add folder shortcut to ur own drive then run\n",
        "!cp \"/content/drive/MyDrive/ICT3113 Performance Testing and Optimisation/data/\"*.pdf data/"
      ],
      "metadata": {
        "id": "ST6BAAF2JZpS"
      },
      "id": "ST6BAAF2JZpS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PDF Parser and Chunking (Using pdfplumber)\n",
        "1. Extracts words from pdf\n",
        "2. Detects sections\n",
        "3. Returns langchain Document object with page content and metadata containing page number, report name, section, and type of content"
      ],
      "metadata": {
        "id": "I5LFZwzdIF66"
      },
      "id": "I5LFZwzdIF66"
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_section(page, text, min_size_diff=2.0):\n",
        "    \"\"\"\n",
        "    Try to extract the section title from page text using regex patterns or font size, weight.\n",
        "    Handles 10-K, 10Q, Presentations, Investor PDFs and similar formats.\n",
        "    \"\"\"\n",
        "    patterns = [\n",
        "        r\"(Item\\s+\\d+[A-Za-z]?.\\s*[A-Z][^\\n]+)\",  # e.g. \"Item 7. Management’s Discussion...\"\n",
        "        r\"(ITEM\\s+\\d+[A-Z]?.\\s*[A-Z][^\\n]+)\",     # uppercase variant\n",
        "        r\"(^[A-Z][A-Z\\s]{10,})\"                   # fallback for all-caps section titles\n",
        "    ]\n",
        "    for p in patterns:\n",
        "        m = re.search(p, text)\n",
        "        if m:\n",
        "            return m.group(1).strip()\n",
        "\n",
        "    # For presentation decks/investor pdfs\n",
        "    try:\n",
        "        words = page.extract_words(extra_attrs=[\"size\", \"fontname\"])\n",
        "        if not words:\n",
        "            return None\n",
        "\n",
        "        from collections import defaultdict\n",
        "        lines = defaultdict(list)\n",
        "\n",
        "        # Group words by y-position\n",
        "        for w in words:\n",
        "            lines[round(w[\"top\"], -1)].append(w)\n",
        "\n",
        "        # Compute global average font size to compare against\n",
        "        avg_font_size = sum(float(w[\"size\"]) for w in words) / len(words)\n",
        "\n",
        "        # Define line scoring function\n",
        "        def score_line(line):\n",
        "            avg_size = sum(float(w[\"size\"]) for w in line) / len(line)\n",
        "            bold_bonus = any(\"Bold\" in w[\"fontname\"] or \"Black\" in w[\"fontname\"] for w in line)\n",
        "            return avg_size + (3 if bold_bonus else 0), avg_size\n",
        "\n",
        "        # Score all lines\n",
        "        scored_lines = [(y, line, *score_line(line)) for y, line in lines.items()]\n",
        "        if not scored_lines:\n",
        "            return None\n",
        "\n",
        "        # Pick the top scoring line\n",
        "        _, best_line, best_score, best_size = max(scored_lines, key=lambda x: x[2])\n",
        "\n",
        "        # If the line isn't significantly larger or bold, discard it\n",
        "        if best_size < avg_font_size + min_size_diff:\n",
        "            # (e.g. all text is 10pt, best line is 11pt → not a section)\n",
        "            return None\n",
        "\n",
        "        # Skip lines that look too long (to avoid full sentences)\n",
        "        if len(best_line) > 20:\n",
        "            return None\n",
        "\n",
        "        section_title = \" \".join(w[\"text\"] for w in best_line)\n",
        "        return section_title.strip() if section_title else None\n",
        "\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def parse_pdf_with_tables(pdf_path, report_name, year=None):\n",
        "    text_docs, table_docs = [], []\n",
        "    current_section = None  # rolling context\n",
        "\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for i, page in enumerate(pdf.pages, start=1):\n",
        "            text = page.extract_text() or \"\"\n",
        "            tables = page.extract_tables() or []\n",
        "\n",
        "            # --- Try to detect section header ---\n",
        "            section = detect_section(page, text)\n",
        "            if section:\n",
        "                current_section = section  # update rolling section\n",
        "\n",
        "            # --- Create text Document ---\n",
        "            text_docs.append(\n",
        "                Document(\n",
        "                    page_content=text,\n",
        "                    metadata={\n",
        "                        \"page\": i,\n",
        "                        \"report\": report_name,\n",
        "                        \"year\": year,\n",
        "                        \"section\": current_section,\n",
        "                        \"type\": \"text\"\n",
        "                    }\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # --- Create table Documents ---\n",
        "            for t in tables:\n",
        "                cleaned_table = [\n",
        "                    [cell if cell is not None else \"\" for cell in row]\n",
        "                    for row in t\n",
        "                ]\n",
        "                table_text = \"\\n\".join([\"\\t\".join(row) for row in cleaned_table])\n",
        "                table_docs.append(\n",
        "                    Document(\n",
        "                        page_content=table_text,\n",
        "                        metadata={\n",
        "                            \"page\": i,\n",
        "                            \"report\": report_name,\n",
        "                            \"year\": year,\n",
        "                            \"section\": current_section,\n",
        "                            \"type\": \"table\"\n",
        "                        }\n",
        "                    )\n",
        "                )\n",
        "\n",
        "    return text_docs, table_docs"
      ],
      "metadata": {
        "id": "tuOHUEdWIjc2"
      },
      "id": "tuOHUEdWIjc2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main ingestion pipeline\n",
        "\n",
        "1.   Loops over all pdfs in folder\n",
        "2.   Checks cache for parsed pdf\n",
        "3.   Load from cache if exists, avoiding re-parsing pdf\n",
        "4.   Save to cache after parsing\n",
        "5.   Use parallel processing to parse pdfs in parallel\n",
        "\n"
      ],
      "metadata": {
        "id": "8HL2Ve2uhwDA"
      },
      "id": "8HL2Ve2uhwDA"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Parameters ---\n",
        "pdf_folder = \"data/\"\n",
        "\n",
        "PARSED_DIR = \"cache/parsed_pdfs\"\n",
        "os.makedirs(PARSED_DIR, exist_ok=True)\n",
        "\n",
        "def parse_pdf_cached(pdf_path, report_name, year=None):\n",
        "    cache_file = os.path.join(PARSED_DIR, f\"{report_name}.json\")\n",
        "\n",
        "    try:\n",
        "        # --- If cached and file not modified, skip re-parsing ---\n",
        "        pdf_mtime = os.path.getmtime(pdf_path)\n",
        "        if os.path.exists(cache_file):\n",
        "            # cache_mtime = os.path.getmtime(cache_file)\n",
        "            # if cache_mtime > pdf_mtime:\n",
        "            print(f\"Using cached parse for {report_name}\\n\")\n",
        "            with open(cache_file, \"r\", encoding=\"utf-8\") as f:\n",
        "                data = json.load(f)\n",
        "            docs = [Document(**d) for d in data]\n",
        "            return docs\n",
        "\n",
        "        # --- Otherwise, parse fresh ---\n",
        "        print(f\"Parsing {os.path.basename(pdf_path)} ...\\n\")\n",
        "        text_docs, table_docs = parse_pdf_with_tables(pdf_path, report_name, year)\n",
        "        docs = text_docs + table_docs\n",
        "\n",
        "        # Save to cache\n",
        "        serializable_docs = [\n",
        "            {\"page_content\": d.page_content, \"metadata\": d.metadata}\n",
        "            for d in docs\n",
        "        ]\n",
        "        with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(serializable_docs, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        return docs\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing PDF {report_name}: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "# --- Measure time!! ----\n",
        "start_total = time.perf_counter()\n",
        "all_docs = []\n",
        "\n",
        "def process_pdf(pdf_path):\n",
        "    report_name = os.path.basename(pdf_path).replace(\".pdf\", \"\")\n",
        "    return parse_pdf_cached(pdf_path, report_name)\n",
        "\n",
        "with ProcessPoolExecutor() as executor:\n",
        "    results = list(executor.map(process_pdf, glob.glob(os.path.join(pdf_folder, \"*.pdf\"))))\n",
        "    for docs in results:\n",
        "        all_docs.extend(docs)\n",
        "\n",
        "elapsed_total = time.perf_counter() - start_total\n",
        "print(f\"Loaded {len(all_docs)} parsed documents in {elapsed_total:.2f}s.\")\n"
      ],
      "metadata": {
        "id": "6c3m1Jvfhxyb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5119b3f5-7f6e-4ee9-fa0b-495e170c2b27"
      },
      "id": "6c3m1Jvfhxyb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing FY25Q2_10Q.pdf ...\n",
            "Parsing FY24_10K.pdf ...\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chunking\n",
        "1. Uses langchain's Recursive Character Text splitter to split documents\n",
        "2. Returns chunked documents"
      ],
      "metadata": {
        "id": "4y-wQYGIe-DE"
      },
      "id": "4y-wQYGIe-DE"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyinstrument import Profiler\n",
        "prof = Profiler(async_mode=\"disabled\")\n",
        "prof.start()\n",
        "\n",
        "start_total = time.perf_counter()\n",
        "\n",
        "max_chunk_size = 800\n",
        "chunk_overlap = 100\n",
        "\n",
        "def chunk_documents(all_docs, chunk_size=max_chunk_size, chunk_overlap=chunk_overlap):\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap\n",
        "    )\n",
        "    return splitter.split_documents(all_docs)\n",
        "\n",
        "chunked_docs = chunk_documents(all_docs, max_chunk_size, chunk_overlap)\n",
        "print(f\"Chunked into {len(chunked_docs)} total segments.\")\n",
        "\n",
        "print(f\"Parsed and chunked {len(all_docs)} total text segments from {len(glob.glob(pdf_folder + '*.pdf'))} PDFs.\")\n",
        "elapsed_total = time.perf_counter() - start_total\n",
        "\n",
        "prof.stop()\n",
        "print(prof.output_text(unicode=True, color=True))\n",
        "prof.last_session.save(\"01_Chunking.pyisession\")\n",
        "html_output = prof.output_html()\n",
        "with open(\"01_Chunking.html\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(html_output)\n",
        "print(f\"Total elapsed time: {elapsed_total}\")"
      ],
      "metadata": {
        "id": "Y2RMsExxeFX6"
      },
      "id": "Y2RMsExxeFX6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6ffb05fc",
      "metadata": {
        "id": "6ffb05fc"
      },
      "source": [
        "## 4. Baseline Pipeline\n",
        "\n",
        "**Baseline (starting point)**\n",
        "*   Naive chunking.\n",
        "*   Single-pass vector search.\n",
        "*   One LLM call, no caching."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "540b7020",
      "metadata": {
        "id": "540b7020"
      },
      "outputs": [],
      "source": [
        "# # TODO: Implement baseline retrieval + generation\n",
        "# # Take a query -> Retrieve relevant chunks -> Feed to LLM -> answer\n",
        "\n",
        "# # number of docs to retrieve per query\n",
        "# bm25_k = 10\n",
        "# faiss_k = 12\n",
        "\n",
        "# # FAISS Vector Index\n",
        "# faiss_store = FAISS.from_documents(chunked_docs, OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n",
        "# faiss_store.save_local(\"faiss_index\")\n",
        "\n",
        "# print(\"FAISS vector index built and saved locally.\")\n",
        "\n",
        "# # BM25 Keyword Index\n",
        "# bm25_retriever = BM25Retriever.from_documents(chunked_docs)\n",
        "# bm25_retriever.k = bm25_k\n",
        "\n",
        "# print(\"BM25 keyword retriever ready.\")\n",
        "\n",
        "# print(\"Ingestion complete — FAISS and BM25 indices are ready.\\n\")\n",
        "\n",
        "# # Test Prompt\n",
        "# question = \"Show Operating Expenses for the last 3 fiscal years, year-on-year comparison.\"\n",
        "\n",
        "# # Test FAISS vector index\n",
        "# faiss_retriever = faiss_store.as_retriever(search_kwargs={\"k\": faiss_k})\n",
        "# test_faiss_docs = faiss_retriever.invoke(question)\n",
        "# print('Using FAISS')\n",
        "# print(test_faiss_docs[0].page_content)\n",
        "# print(test_faiss_docs[0].metadata)\n",
        "# print('\\n')\n",
        "\n",
        "# # Test BM25 retriever\n",
        "# test_bm25_docs = bm25_retriever.invoke(question)\n",
        "# print('Using BM25')\n",
        "# print(test_bm25_docs[0].page_content)\n",
        "# print(test_bm25_docs[0].metadata)\n",
        "# print('\\n')\n",
        "\n",
        "# # Test SVM retriever\n",
        "# # svm_retriever = SVMRetriever.from_documents(chunked_docs, embeddings=embedding_model)\n",
        "# # svm_retriever.k = 10\n",
        "# # test_svm_docs = svm_retriever.invoke(question)\n",
        "# # print('Using SVM:')\n",
        "# # print(test_svm_docs[0].page_content)\n",
        "# # print(test_svm_docs[0].metadata)\n",
        "\n",
        "# context = \"\\n\\n\".join([doc.page_content for doc in test_faiss_docs])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for doc in test_faiss_docs:\n",
        "#   print(\"============================\")\n",
        "#   print(doc.page_content)\n",
        "#   print(doc.metadata)"
      ],
      "metadata": {
        "id": "YnSosej0qY67"
      },
      "id": "YnSosej0qY67",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Set Prompts\n",
        "# from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# prompt_template = \\\n",
        "# \"\"\"\n",
        "# You are a helpful assistant for question answering tasks.\n",
        "# Use the following pieces of retrieved context to answer the given question.\n",
        "# If you dont know the answer, just say that you dont know.\n",
        "# Use up to three sentenses to keep answer precise.\n",
        "\n",
        "# Question: {question}\n",
        "\n",
        "# Context: {context}\n",
        "# \"\"\"\n",
        "\n",
        "# prompt_template = PromptTemplate.from_template(prompt_template)\n",
        "# # Invoke and pass in query and context\n",
        "# prompt = prompt_template.invoke(\n",
        "#     {\n",
        "#         \"context\": context,\n",
        "#         \"question\": question\n",
        "#     }\n",
        "# )\n",
        "# prompt.text"
      ],
      "metadata": {
        "id": "D-3sC_53yVup"
      },
      "id": "D-3sC_53yVup",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Generate Response\n",
        "# llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "# response = llm.invoke(prompt)\n",
        "\n",
        "# # Parse the response into well formated text\n",
        "# from langchain_core.output_parsers import StrOutputParser\n",
        "# parser = StrOutputParser()\n",
        "\n",
        "# res = parser.invoke(response)\n",
        "\n",
        "# print(res)"
      ],
      "metadata": {
        "id": "yc15hssb1rXj"
      },
      "id": "yc15hssb1rXj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4b. Agentic RAG"
      ],
      "metadata": {
        "id": "WzF8C-ivUxmq"
      },
      "id": "WzF8C-ivUxmq"
    },
    {
      "cell_type": "code",
      "source": [
        "# Retriever Tool\n",
        "class RetrieverTool:\n",
        "    \"\"\"Wrapper around FAISS retriever for LangChain/OpenAI tools interface.\"\"\"\n",
        "\n",
        "    def __init__(self, faiss_store: FAISS, top_k: int = 12):\n",
        "        self.retriever: BaseRetriever = faiss_store.as_retriever(\n",
        "            search_kwargs={\"k\": top_k}\n",
        "        )\n",
        "\n",
        "    def forward(self, query: str) -> str:\n",
        "        \"\"\"Retrieve top-k documents similar to the query.\"\"\"\n",
        "        assert isinstance(query, str), \"Your search query must be a string\"\n",
        "        docs = self.retriever.invoke(query)\n",
        "\n",
        "        if not docs:\n",
        "            return \"No relevant documents found.\"\n",
        "\n",
        "        formatted_docs = \"\\n\".join(\n",
        "            [\n",
        "                f\"===== Document {i+1} =====\\n\"\n",
        "                f\"{doc.page_content}\\n\"\n",
        "                f\"Metadata: {doc.metadata}\\n\"\n",
        "                for i, doc in enumerate(docs)\n",
        "            ]\n",
        "        )\n",
        "        return formatted_docs\n",
        "\n",
        "    def as_tool(self) -> StructuredTool:\n",
        "        \"\"\"Convert to a LangChain StructuredTool usable by OpenAI agents.\"\"\"\n",
        "        return StructuredTool.from_function(\n",
        "            func=self.forward,\n",
        "            name=\"retriever\",\n",
        "            description=\"Retrieves semantically similar documents from a FAISS vector index given a natural language query.\",\n",
        "        )"
      ],
      "metadata": {
        "id": "znVMT4r8LvCs"
      },
      "id": "znVMT4r8LvCs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculator Tool for comparisons\n",
        "class CalculatorTool:\n",
        "    \"\"\"Tool to compute YoY or QoQ percentage changes from numeric data.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(input_str: str) -> str:\n",
        "        \"\"\"\n",
        "        Accepts a JSON string like:\n",
        "        {\n",
        "          \"data\": [{\"fiscal_year\": \"2022\", \"opex\": 7434.0}, ...],\n",
        "          \"metric_key\": \"opex\",\n",
        "          \"period_key\": \"fiscal_year\",\n",
        "          \"comparison_type\": \"yoy\"\n",
        "        }\n",
        "        Returns: JSON string with computed results.\n",
        "        \"\"\"\n",
        "        # Clean up formatting issues from the LLM\n",
        "        input_str = input_str.strip().strip(\"`\").strip(\"'\").strip('\"')\n",
        "\n",
        "        try:\n",
        "            params = json.loads(input_str)\n",
        "        except Exception as e:\n",
        "            return json.dumps({\"error\": f\"Invalid input JSON: {str(e)}\", \"received\": input_str})\n",
        "\n",
        "        # Input Structure\n",
        "        data = params.get(\"data\", [])\n",
        "        metric_key = params.get(\"metric_key\", \"opex\")\n",
        "        period_key = params.get(\"period_key\", \"fiscal_year\")\n",
        "        comparison_type = params.get(\"comparison_type\", \"yoy\")\n",
        "\n",
        "        # Handle dictionary-shaped data (like {\"2022\": 7434, ...})\n",
        "        if isinstance(data, dict):\n",
        "            data = [{period_key: k, metric_key: v} for k, v in data.items()]\n",
        "\n",
        "        if not isinstance(data, list):\n",
        "            return json.dumps({\"error\": \"Expected 'data' to be a list or dict of year:value pairs.\"})\n",
        "\n",
        "        # Compute percentage changes\n",
        "        results = []\n",
        "        for i, record in enumerate(data):\n",
        "            if i == 0:\n",
        "                pct_change = None\n",
        "            else:\n",
        "                prev_value = data[i - 1][metric_key]\n",
        "                curr_value = record[metric_key]\n",
        "                pct_change = ((curr_value - prev_value) / prev_value) * 100\n",
        "\n",
        "            results.append({\n",
        "                period_key: record[period_key],\n",
        "                metric_key: record[metric_key],\n",
        "                f\"{comparison_type}_change (%)\": round(pct_change, 2) if pct_change is not None else None,\n",
        "                \"units\": record.get(\"units\", \"millions USD\"),\n",
        "            })\n",
        "\n",
        "        return json.dumps(results, indent=2)\n",
        "\n",
        "    def as_tool(self) -> StructuredTool:\n",
        "        return StructuredTool.from_function(\n",
        "            func=self.forward,\n",
        "            name=\"calculator\",\n",
        "            description=(\n",
        "                \"Computes YoY or QoQ percentage changes. Input should be a JSON string containing data and comparison details.\"\n",
        "            ),\n",
        "        )"
      ],
      "metadata": {
        "id": "Hrdf2vVdL0Tj"
      },
      "id": "Hrdf2vVdL0Tj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "# os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('LANGSMITH_KEY')\n",
        "# os.environ[\"LANGCHAIN_PROJECT\"] = \"AgentPerformance\""
      ],
      "metadata": {
        "id": "m6OvLbhUgoLq"
      },
      "id": "m6OvLbhUgoLq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_faiss_index(chunked_docs):\n",
        "  embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\") # Can be 'text-embedding-3-large'\n",
        "\n",
        "  # FAISS Vector Index\n",
        "  faiss_store = FAISS.from_documents(chunked_docs, embedding_model)\n",
        "  faiss_store.save_local(\"faiss_index\")\n",
        "\n",
        "  # Load FAISS Vector Store\n",
        "  faiss_store = FAISS.load_local(\n",
        "      \"faiss_index\",\n",
        "      embeddings=embedding_model,\n",
        "      allow_dangerous_deserialization=True\n",
        "  )\n",
        "  return faiss_store"
      ],
      "metadata": {
        "id": "1QekVw22jrds"
      },
      "id": "1QekVw22jrds",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_agent(faiss_store):\n",
        "  # Create Tools\n",
        "  retriever_tool = RetrieverTool(faiss_store=faiss_store, top_k=12).as_tool()\n",
        "  calculator_tool = CalculatorTool().as_tool()\n",
        "\n",
        "  # Initialize OpenAI LLM\n",
        "  openai_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "  # Create OpenAI-compatible agent that can use tools\n",
        "  agent = initialize_agent(\n",
        "      tools=[retriever_tool, calculator_tool],\n",
        "      llm=openai_llm,\n",
        "      agent_type=AgentType.OPENAI_FUNCTIONS,  # enables OpenAI’s function/tool calling\n",
        "      verbose=True,\n",
        "      handle_parsing_errors=True,\n",
        "  )\n",
        "  return agent"
      ],
      "metadata": {
        "id": "rbuOedjMkAkP"
      },
      "id": "rbuOedjMkAkP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompt(question):\n",
        "  return f\"\"\"\n",
        "You are a financial analyst agent that can use tools.\n",
        "\n",
        "Use the retriever tool when you need to fetch financial data. You can only call this tool ONCE.\n",
        "Use the calculator tool when you need to compute YoY or QoQ comparisons.\n",
        "\n",
        "Once done, return the **final structured JSON output** in this format:\n",
        "\n",
        "{{\n",
        "\"query\": \"...\",\n",
        "\"data_values\": [...],\n",
        "\"computed_values\": [...],\n",
        "\"citations\": [<list the report, page and section used>],\n",
        "\"tools\": [\"<list the tools you actually used>\"],\n",
        "\"tools_count\": <total number of tools used>\n",
        "}}\n",
        "\n",
        "Guidelines:\n",
        "- `data_values` contain the raw financial figures, corresponding fiscal years, and units retrieved directly from reports before any calculations.\n",
        "- `computed_values` include the calculated results (e.g., YoY or QoQ changes) together with the corresponding values from data_values.\n",
        "- Always include every period in `computed_values`, even if the change value is null.\n",
        "\n",
        "Now, handle this query:\n",
        "{question}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "3QkslaOPjUeH"
      },
      "id": "3QkslaOPjUeH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.callbacks.base import BaseCallbackHandler\n",
        "\n",
        "class TimingCallback(BaseCallbackHandler):\n",
        "    def __init__(self):\n",
        "        self.tool_timings = []\n",
        "        self.current_tool = None\n",
        "        self.start_time = None\n",
        "        self.llm_start = None\n",
        "        self.reasoning_time = None\n",
        "\n",
        "    def on_tool_start(self, serialized, input_str, **kwargs):\n",
        "        self.current_tool = serialized.get(\"name\", \"UnknownTool\")\n",
        "        self.start_time = time.perf_counter()\n",
        "        print(f\"Tool start: {self.current_tool}\")\n",
        "\n",
        "    def on_tool_end(self, output, **kwargs):\n",
        "        duration = time.perf_counter() - self.start_time\n",
        "        print(f\"Tool end: {self.current_tool} ({duration:.3f}s)\")\n",
        "        self.tool_timings.append((self.current_tool, duration))\n",
        "        self.current_tool = None\n",
        "\n",
        "    def on_llm_start(self, *args, **kwargs):\n",
        "        self.llm_start = time.perf_counter()\n",
        "\n",
        "    def on_llm_end(self, response, **kwargs):\n",
        "        if self.llm_start:\n",
        "            duration = time.perf_counter() - self.llm_start\n",
        "            print(f\"LLM reasoning/generation: {duration:.3f}s\")\n",
        "            self.reasoning_time = duration"
      ],
      "metadata": {
        "id": "tGZJkRBE1ien"
      },
      "id": "tGZJkRBE1ien",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prof = Profiler(async_mode=\"disabled\")\n",
        "prof.start()\n",
        "timing_callback = TimingCallback()\n",
        "\n",
        "start_total = time.perf_counter()\n",
        "\n",
        "faiss_store = build_faiss_index(chunked_docs)\n",
        "\n",
        "agent = create_agent(faiss_store)\n",
        "\n",
        "# Run the Agent with a Query\n",
        "question = \"Show Operating Expenses for the last 3 fiscal years, year-on-year comparison.\"\n",
        "final_prompt = create_prompt(question)\n",
        "response = agent.invoke({\"input\": final_prompt}, config={\"callbacks\": [timing_callback]})\n",
        "print(response)\n",
        "\n",
        "elapsed_total = time.perf_counter() - start_total\n",
        "\n",
        "prof.stop()\n",
        "print(prof.output_text(unicode=True, color=True))\n",
        "prof.last_session.save(\"02_Agent.pyisession\")\n",
        "html_output = prof.output_html()\n",
        "with open(\"02_Agent.html\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(html_output)\n",
        "print(f\"Total elapsed time: {elapsed_total}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmfIuSUHPjmV",
        "outputId": "fa8052c9-ab6f-4886-8345-42edb3a5437c"
      },
      "id": "NmfIuSUHPjmV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3888236092.py:10: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
            "  agent = initialize_agent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "LLM reasoning/generation: 2.472s\n",
            "\u001b[32;1m\u001b[1;3mTo answer the question about Operating Expenses for the last 3 fiscal years and provide a year-on-year comparison, I will first need to retrieve the relevant financial data using the retriever tool.\n",
            "\n",
            "Action: retriever  \n",
            "Action Input: \"Operating Expenses for the last 3 fiscal years year-on-year comparison\"\n",
            "\u001b[0mTool start: retriever\n",
            "Tool end: retriever (0.402s)\n",
            "\n",
            "Observation: \u001b[36;1m\u001b[1;3m===== Document 1 =====\n",
            "Table of Contents\n",
            "Operating Expenses\n",
            "Year Ended\n",
            "January 29, January 30, $ %\n",
            "2023 2022 Change Change\n",
            "($ in millions)\n",
            "Research and development expenses $ 7,339 $ 5,268 $ 2,071 39 %\n",
            "% of revenue 27.2 % 19.6 %\n",
            "Sales, general and administrative expenses 2,440 2,166 274 13 %\n",
            "% of revenue 9.1 % 8.0 %\n",
            "Acquisition termination cost 1,353 — 1,353 100 %\n",
            "% of revenue 5.0 % — %\n",
            "Total operating expenses $ 11,132 $ 7,434 $ 3,698 50 %\n",
            "% of revenue 41.3 % 27.6 %\n",
            "The increase in research and development expense for fiscal year 2023 was primarily driven by increased compensation, employee growth,\n",
            "engineering development costs, and data center infrastructure.\n",
            "The increase in sales, general and administrative expense for fiscal year 2023 was primarily driven by increased compensation and employee\n",
            "growth.\n",
            "Metadata: {'page': 43, 'report': 'FY23_10K', 'year': None, 'section': 'Item 1A. Risk', 'type': 'text'}\n",
            "\n",
            "===== Document 2 =====\n",
            "O perating Expenses\n",
            "Three Months E nded Nine Months Ended\n",
            "October 29, October 30, $ % October 29, October 30, $ %\n",
            "2023 2022 Change Change 2023 2022 Change Change\n",
            "($ in millio ns)\n",
            "Research and\n",
            "development expenses $ 2,294 $ 1,945 $ 349 18 % $ 6,210 $ 5,387 $ 823 15 %\n",
            "% of net revenue 12.7 % 32.8 % 16.0 % 25.7 %\n",
            "Sales, general and\n",
            "administrative expenses 689 631 58 9 % 1,942 1,815 127 7 %\n",
            "% of net revenue 3.8 % 10.6 % 5.0 % 8.7 %\n",
            "Acquisition termination\n",
            "cost — — — — % — 1,353 (1,353) (100) %\n",
            "% of net revenue — % — % — % 6.5 %\n",
            "Total operating\n",
            "expenses $ 2,983 $ 2,576 $ 407 16 % $ 8,152 $ 8,555 $ (403) (5) %\n",
            "% of net revenue 16.5 % 43.4 % 21.0 % 40.9 %\n",
            "The increases in research and development expenses and sales, general and administrative expenses for the third quarter and first nine months\n",
            "Metadata: {'page': 31, 'report': 'FY24Q3_10Q', 'year': None, 'section': 'ITEM 2. MANAGEMENT’S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS', 'type': 'text'}\n",
            "\n",
            "===== Document 3 =====\n",
            "Operating Expenses\n",
            "Year Ended\n",
            "$ %\n",
            "Jan 28, 2024 Jan 29, 2023 Change Change\n",
            "($ in millions)\n",
            "Research and development expenses $ 8,675 $ 7,339 $ 1,336 18 %\n",
            "% of net revenue 14.2 % 27.2 %\n",
            "Sales, general and administrative expenses 2,654 2,440 214 9 %\n",
            "% of net revenue 4.4 % 9.1 %\n",
            "Acquisition termination cost — 1,353 (1,353) (100)%\n",
            "% of net revenue — % 5.0 %\n",
            "Total operating expenses $ 11,329 $ 11,132 $ 197 2 %\n",
            "% of net revenue 18.6 % 41.3 %\n",
            "The increase in research and development expenses and sales, general and administrative expenses for fiscal year 2024 was primarily driven by compensation\n",
            "and benefits, including stock-based compensation, reflecting employee growth and compensation increases.\n",
            "Acquisition Termination Cost\n",
            "Metadata: {'page': 40, 'report': 'FY24_10K', 'year': None, 'section': 'Item 1A. Risk Factors', 'type': 'text'}\n",
            "\n",
            "===== Document 4 =====\n",
            "Operating income (loss)\t$\t10,262\t\t\t$\t1,493\t\t$\t(1,338)\t\t$\t10,417\t\n",
            "Three Months Ended October 30, 2022\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
            "Revenue\t$\t3,816\t\t\t$\t2,115\t\t$\t—\t\t$\t5,931\n",
            "Metadata: {'page': 23, 'report': 'FY24Q3_10Q', 'year': None, 'section': 'NVIDIA CORPORATION AND SUBSIDIARIES\\nNOTES TO CONDENSED CONSOLIDATED FINANCIAL STATEMENTS', 'type': 'table'}\n",
            "\n",
            "===== Document 5 =====\n",
            "Critical Accounting Policies and Estimates\n",
            "Refer to Part II, Item 7, \"Critical Accounting Policies and Estimates\" of our Annual Report on Form 10-K for the fiscal year ended January 29,\n",
            "2023. There have been no material changes to our Critical Accounting Policies and Estimates.\n",
            "Results of Operations\n",
            "The following table sets forth, for the periods indicated, certain items in our Condensed Consolidated Statements of Income expressed as a\n",
            "percentage of revenue.\n",
            "Three Mon ths Ended Nine Mont hs Ended\n",
            "October 29, October 30, October 29, October 30,\n",
            "2023 2022 2023 2022\n",
            "Revenue 100.0 % 100.0 % 100.0 % 100.0 %\n",
            "Cost of revenue 26.0 46.4 29.1 44.9\n",
            "Gross profit 74.0 53.6 70.9 55.1\n",
            "Operating expenses\n",
            "Research and development 12.7 32.8 16.0 25.7\n",
            "Sales, general and administrative 3.8 10.6 5.0 8.7\n",
            "Metadata: {'page': 29, 'report': 'FY24Q3_10Q', 'year': None, 'section': 'ITEM 2. MANAGEMENT’S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS', 'type': 'text'}\n",
            "\n",
            "===== Document 6 =====\n",
            "Three Months Ended Jul 28, 2024\t\t\t\t\t\t\t\t\t\t\t\n",
            "Revenue\t$\t26,446\t\t$\t3,594\t\t$\t—\t\t$\t30,040\n",
            "Operating income (loss)\t$\t18,848\t\t$\t1,369\t\t$\t(1,575)\t\t$\t18,642\n",
            "\t\t\t\t\t\t\t\t\t\t\t\n",
            "Three Months Ended Jul 30, 2023\t\t\t\t\t\t\t\t\t\t\t\n",
            "Revenue\t$\t10,402\t\t$\t3,105\t\t$\t—\t\t$\t13,507\n",
            "Operating income (loss)\t$\t6,728\t\t$\t1,211\t\t$\t(1,139)\t\t$\t6,800\n",
            "\t\t\t\t\t\t\t\t\t\t\t\n",
            "Six Months Ended Jul 28, 2024\t\t\t\t\t\t\t\t\t\t\t\n",
            "Revenue\t$\t49,121\t\t$\t6,963\t\t$\t—\t\t$\t56,084\n",
            "Operating income (loss)\t$\t35,896\t\t$\t2,609\t\t$\t(2,954)\t\t$\t35,551\n",
            "\t\t\t\t\t\t\t\t\t\t\t\n",
            "Six Months Ended Jul 30, 2023\t\t\t\t\t\t\t\t\t\t\t\n",
            "Revenue\t$\t14,862\t\t$\t5,837\t\t$\t—\t\t$\t20,699\n",
            "Operating income (loss)\t$\t8,887\t\t$\t2,258\t\t$\t(2,204)\t\t$\t8,941\n",
            "Metadata: {'page': 21, 'report': 'FY25Q2_10Q', 'year': None, 'section': 'Item 1. Financial Statements (Unaudited)', 'type': 'table'}\n",
            "\n",
            "===== Document 7 =====\n",
            "Table of Contents\n",
            "Operating Expenses\n",
            "Year Ended\n",
            "January 30, January 31, $ %\n",
            "2022 2021 Change Change\n",
            "($ in millions)\n",
            "Research and development expenses $ 5,268 $ 3,924 $ 1,344 34 %\n",
            "% of net revenue 19.6 % 23.5 %\n",
            "Sales, general and administrative expenses 2,166 1,940 226 12 %\n",
            "% of net revenue 8.0 % 11.6 %\n",
            "Total operating expenses $ 7,434 $ 5,864 $ 1,570 27 %\n",
            "Research and Development\n",
            "Research and development expenses increased by 34% in fiscal year 2022 compared to fiscal year 2021, primarily driven by stock-based\n",
            "compensation, compensation-related costs associated with employee growth and higher infrastructure costs.\n",
            "Sales, General and Administrative\n",
            "Sales, general and administrative expenses increased by 12% in fiscal year 2022 compared to fiscal year 2021, primarily driven by stock-based\n",
            "Metadata: {'page': 37, 'report': 'FY22_10K', 'year': None, 'section': 'Item 1A. Risk Factors”,', 'type': 'text'}\n",
            "\n",
            "===== Document 8 =====\n",
            "Table of Contents\n",
            "The following table sets forth, for the periods indicated, certain items in our Consolidated Statements of Income expressed as a percentage of revenue.\n",
            "Year Ended\n",
            "Jan 28, 2024 Jan 29, 2023\n",
            "Revenue 100.0 % 100.0 %\n",
            "Cost of revenue 27.3 43.1\n",
            "Gross profit 72.7 56.9\n",
            "Operating expenses\n",
            "Research and development 14.2 27.2\n",
            "Sales, general and administrative 4.4 9.1\n",
            "Acquisition termination cost — 5.0\n",
            "Total operating expenses 18.6 41.3\n",
            "Operating income 54.1 15.6\n",
            "Interest income 1.4 1.0\n",
            "Interest expense (0.4) (1.0)\n",
            "Other, net 0.4 (0.1)\n",
            "Other income (expense), net 1.4 (0.1)\n",
            "Income before income tax 55.5 15.5\n",
            "Income tax expense (benefit) 6.6 (0.7)\n",
            "Net income 48.9 % 16.2 %\n",
            "Reportable Segments\n",
            "Revenue by Reportable Segments\n",
            "Year Ended\n",
            "$ %\n",
            "Jan 28, 2024 Jan 29, 2023 Change Change\n",
            "Metadata: {'page': 39, 'report': 'FY24_10K', 'year': None, 'section': 'Item 1A. Risk Factors', 'type': 'text'}\n",
            "\n",
            "===== Document 9 =====\n",
            "Year Ended January 29, 2023:\t\t\t\t\t\t\t\t\n",
            "Revenue\t$\t15,068\t$\t11,906\t$\t—\t$\t26,974\n",
            "Operating income (loss)\t$\t5,083\t$\t4,552\t$\t(5,411)\t$\t4,224\n",
            "\t\t\t\t\t\t\t\t\n",
            "Year Ended January 30, 2022:\t\t\t\t\t\t\t\t\n",
            "Revenue\t$\t11,046\t$\t15,868\t$\t—\t$\t26,914\n",
            "Operating income (loss)\t$\t4,598\t$\t8,492\t$\t(3,049)\t$\t10,041\n",
            "\t\t\t\t\t\t\t\t\n",
            "Year Ended January 31, 2021:\t\t\t\t\t\t\t\t\n",
            "Revenue\t$\t6,841\t$\t9,834\t$\t—\t$\t16,675\n",
            "Operating income (loss)\t$\t2,548\t$\t4,612\t$\t(2,628)\t$\t4,532\n",
            "Metadata: {'page': 85, 'report': 'FY23_10K', 'year': None, 'section': 'NVIDIA CORPORATION AND SUBSIDIARIES', 'type': 'table'}\n",
            "\n",
            "===== Document 10 =====\n",
            "2023. There have been no material changes to our Critical Accounting Policies and Estimates.\n",
            "Results of Operations\n",
            "The following table sets forth, for the periods indicated, certain items in our Condensed Consolidated Statements of Income expressed as a\n",
            "percentage of revenue.\n",
            "Three Months Ended Six Months Ended\n",
            "July 30, July 31, July 30, July 31,\n",
            "2023 2022 2023 2022\n",
            "Revenue 100.0 % 100.0 % 100.0 % 100.0 %\n",
            "Cost of revenue 29.9 56.5 31.8 44.3\n",
            "Gross profit 70.1 43.5 68.2 55.7\n",
            "Operating expenses\n",
            "Research and development 15.1 27.2 18.9 23.0\n",
            "Sales, general and administrative 4.7 8.8 6.1 7.9\n",
            "Acquisition termination cost — — — 9.0\n",
            "Total operating expenses 19.8 36.0 25.0 39.9\n",
            "Operating income 50.3 7.5 43.2 15.8\n",
            "Interest income 1.4 0.7 1.6 0.4\n",
            "Interest expense (0.5) (1.0) (0.6) (0.9)\n",
            "Metadata: {'page': 28, 'report': 'FY24Q2_10Q', 'year': None, 'section': 'ITEM 2. MANAGEMENT’S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS', 'type': 'text'}\n",
            "\n",
            "===== Document 11 =====\n",
            "Year Ended Jan 26, 2025\t\t\t\t\t\t\t\t\t\t\t\n",
            "Revenue\t$\t116,193\t\t$\t14,304\t\t$\t—\t\t$\t130,497\n",
            "Other segment items (1)\t33,318\t\t\t9,219\t\t\t\t\t\t\t\n",
            "Operating income (loss)\t$\t82,875\t\t$\t5,085\t\t$\t(6,507)\t\t$\t81,453\n",
            "\t\t\t\t\t\t\t\t\t\t\t\n",
            "Year Ended Jan 28, 2024\t\t\t\t\t\t\t\t\t\t\t\n",
            "Revenue\t$\t47,405\t\t$\t13,517\t\t$\t—\t\t$\t60,922\n",
            "Other segment items (1)\t15,389\t\t\t7,671\t\t\t\t\t\t\t\n",
            "Operating income (loss)\t$\t32,016\t\t$\t5,846\t\t$\t(4,890)\t\t$\t32,972\n",
            "\t\t\t\t\t\t\t\t\t\t\t\n",
            "Year Ended Jan 29, 2023\t\t\t\t\t\t\t\t\t\t\t\n",
            "Revenue\t$\t15,068\t\t$\t11,906\t\t$\t—\t\t$\t26,974\n",
            "Other segment items (1)\t9,985\t\t\t7,354\t\t\t\t\t\t\t\n",
            "Operating income (loss)\t$\t5,083\t\t$\t4,552\t\t$\t(5,411)\t\t$\t4,224\n",
            "Metadata: {'page': 78, 'report': 'FY25_10K', 'year': None, 'section': \"Item 9A. Our responsibility is to express opinions on the Company’s consolidated financial statements and on the Company's internal\", 'type': 'table'}\n",
            "\n",
            "===== Document 12 =====\n",
            "Three Months Ended Apr 28, 2024\t\t\t\t\t\t\t\t\t\t\t\n",
            "Revenue\t$\t22,675\t\t$\t3,369\t\t$\t—\t\t$\t26,044\n",
            "Operating income (loss)\t$\t17,047\t\t$\t1,241\t\t$\t(1,379)\t\t$\t16,909\n",
            "\t\t\t\t\t\t\t\t\t\t\t\n",
            "Three Months Ended Apr 30, 2023\t\t\t\t\t\t\t\t\t\t\t\n",
            "Revenue\t$\t4,460\t\t$\t2,732\t\t$\t—\t\t$\t7,192\n",
            "Operating income (loss)\t$\t2,160\t\t$\t1,046\t\t$\t(1,066)\t\t$\t2,140\n",
            "Metadata: {'page': 20, 'report': 'FY25Q1_10Q', 'year': None, 'section': 'Item 1. Financial Statements (Unaudited)', 'type': 'table'}\n",
            "\u001b[0m\n",
            "Thought:LLM reasoning/generation: 5.263s\n",
            "\u001b[32;1m\u001b[1;3mI have retrieved the relevant financial data for Operating Expenses over the last three fiscal years. Now, I will extract the total operating expenses for each year and prepare to calculate the year-on-year changes.\n",
            "\n",
            "The retrieved data shows:\n",
            "- For the year ended January 29, 2023: Total operating expenses = $11,132 million\n",
            "- For the year ended January 30, 2022: Total operating expenses = $7,434 million\n",
            "- For the year ended January 31, 2021: Total operating expenses = $5,864 million\n",
            "\n",
            "Next, I will calculate the year-on-year percentage changes for these operating expenses.\n",
            "\n",
            "Action: calculator  \n",
            "Action Input: '{\"data\": {\"2023\": 11132, \"2022\": 7434, \"2021\": 5864}, \"comparison\": \"YoY\"}'\n",
            "\u001b[0mTool start: calculator\n",
            "Tool end: calculator (0.001s)\n",
            "\n",
            "Observation: \u001b[33;1m\u001b[1;3m[\n",
            "  {\n",
            "    \"fiscal_year\": \"2023\",\n",
            "    \"opex\": 11132,\n",
            "    \"yoy_change (%)\": null,\n",
            "    \"units\": \"millions USD\"\n",
            "  },\n",
            "  {\n",
            "    \"fiscal_year\": \"2022\",\n",
            "    \"opex\": 7434,\n",
            "    \"yoy_change (%)\": -33.22,\n",
            "    \"units\": \"millions USD\"\n",
            "  },\n",
            "  {\n",
            "    \"fiscal_year\": \"2021\",\n",
            "    \"opex\": 5864,\n",
            "    \"yoy_change (%)\": -21.12,\n",
            "    \"units\": \"millions USD\"\n",
            "  }\n",
            "]\u001b[0m\n",
            "Thought:LLM reasoning/generation: 7.960s\n",
            "\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
            "\n",
            "Final Answer: \n",
            "```json\n",
            "{\n",
            "  \"query\": \"Show Operating Expenses for the last 3 fiscal years, year-on-year comparison.\",\n",
            "  \"data_values\": [\n",
            "    {\n",
            "      \"fiscal_year\": \"2023\",\n",
            "      \"opex\": 11132,\n",
            "      \"units\": \"millions USD\"\n",
            "    },\n",
            "    {\n",
            "      \"fiscal_year\": \"2022\",\n",
            "      \"opex\": 7434,\n",
            "      \"units\": \"millions USD\"\n",
            "    },\n",
            "    {\n",
            "      \"fiscal_year\": \"2021\",\n",
            "      \"opex\": 5864,\n",
            "      \"units\": \"millions USD\"\n",
            "    }\n",
            "  ],\n",
            "  \"computed_values\": [\n",
            "    {\n",
            "      \"fiscal_year\": \"2023\",\n",
            "      \"opex\": 11132,\n",
            "      \"yoy_change (%)\": null\n",
            "    },\n",
            "    {\n",
            "      \"fiscal_year\": \"2022\",\n",
            "      \"opex\": 7434,\n",
            "      \"yoy_change (%)\": -33.22\n",
            "    },\n",
            "    {\n",
            "      \"fiscal_year\": \"2021\",\n",
            "      \"opex\": 5864,\n",
            "      \"yoy_change (%)\": -21.12\n",
            "    }\n",
            "  ],\n",
            "  \"citations\": [\n",
            "    \"FY23_10K\",\n",
            "    \"FY22_10K\"\n",
            "  ],\n",
            "  \"tools\": [\"retriever\", \"calculator\"],\n",
            "  \"tools_count\": 2\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{'input': '\\nYou are a financial analyst agent that can use tools.\\n\\nUse the retriever tool when you need to fetch financial data. You can only call this tool ONCE.\\nUse the calculator tool when you need to compute YoY or QoQ comparisons.\\n\\nOnce done, return the **final structured JSON output** in this format:\\n\\n{\\n\"query\": \"...\",\\n\"data_values\": [...],\\n\"computed_values\": [...],\\n\"citations\": [...],\\n\"tools\": [\"<list the tools you actually used>\"],\\n\"tools_count\": <total number of tools used>\\n}\\n\\nGuidelines:\\n- `data_values` contain the raw financial figures, corresponding fiscal years, and units retrieved directly from reports before any calculations.\\n- `computed_values` include the calculated results (e.g., YoY or QoQ changes) together with the corresponding values from data_values.\\n- Always include every period in `computed_values`, even if the change value is null.\\n\\nNow, handle this query:\\nShow Operating Expenses for the last 3 fiscal years, year-on-year comparison.\\n', 'output': '```json\\n{\\n  \"query\": \"Show Operating Expenses for the last 3 fiscal years, year-on-year comparison.\",\\n  \"data_values\": [\\n    {\\n      \"fiscal_year\": \"2023\",\\n      \"opex\": 11132,\\n      \"units\": \"millions USD\"\\n    },\\n    {\\n      \"fiscal_year\": \"2022\",\\n      \"opex\": 7434,\\n      \"units\": \"millions USD\"\\n    },\\n    {\\n      \"fiscal_year\": \"2021\",\\n      \"opex\": 5864,\\n      \"units\": \"millions USD\"\\n    }\\n  ],\\n  \"computed_values\": [\\n    {\\n      \"fiscal_year\": \"2023\",\\n      \"opex\": 11132,\\n      \"yoy_change (%)\": null\\n    },\\n    {\\n      \"fiscal_year\": \"2022\",\\n      \"opex\": 7434,\\n      \"yoy_change (%)\": -33.22\\n    },\\n    {\\n      \"fiscal_year\": \"2021\",\\n      \"opex\": 5864,\\n      \"yoy_change (%)\": -21.12\\n    }\\n  ],\\n  \"citations\": [\\n    \"FY23_10K\",\\n    \"FY22_10K\"\\n  ],\\n  \"tools\": [\"retriever\", \"calculator\"],\\n  \"tools_count\": 2\\n}\\n```'}\n",
            "\n",
            "  _     ._   __/__   _ _  _  _ _/_   Recorded: 10:41:13  Samples:  5832\n",
            " /_//_/// /_\\ / //_// / //_'/ //     Duration: 55.932    CPU time: 8.841\n",
            "/   _/                      v5.1.1\n",
            "\n",
            "Profile at /tmp/ipython-input-1408470266.py:2\n",
            "\n",
            "\u001b[31m55.931\u001b[0m Shell.run_ast_nodes\u001b[0m  \u001b[2mIPython/core/interactiveshell.py:3360\u001b[0m\n",
            "└─ \u001b[31m55.930\u001b[0m \u001b[48;5;24m\u001b[38;5;15m<cell line: 0>\u001b[0m  \u001b[2m/tmp/ipython-input-1408470266.py:1\u001b[0m\n",
            "   ├─ \u001b[31m39.586\u001b[0m \u001b[48;5;24m\u001b[38;5;15mbuild_faiss_index\u001b[0m  \u001b[2m/tmp/ipython-input-3501748242.py:1\u001b[0m\n",
            "   │  ├─ \u001b[31m37.260\u001b[0m FAISS.from_documents\u001b[0m  \u001b[2mlangchain_core/vectorstores/base.py:809\u001b[0m\n",
            "   │  │     [74 frames hidden]  \u001b[2mlangchain_community, langchain_openai...\u001b[0m\n",
            "   │  │        \u001b[33m21.696\u001b[0m _SSLSocket.read\u001b[0m  \u001b[2m<built-in>\u001b[0m\n",
            "   │  └─ \u001b[92m\u001b[2m1.753\u001b[0m OpenAIEmbeddings.validate_environment\u001b[0m  \u001b[2mlangchain_openai/embeddings/base.py:289\u001b[0m\n",
            "   │        [5 frames hidden]  \u001b[2mfunctools, openai\u001b[0m\n",
            "   └─ \u001b[33m16.110\u001b[0m AgentExecutor.invoke\u001b[0m  \u001b[2mlangchain/chains/base.py:128\u001b[0m\n",
            "         [32 frames hidden]  \u001b[2mlangchain, langchain_core, langchain_...\u001b[0m\n",
            "            \u001b[33m15.583\u001b[0m _SSLSocket.read\u001b[0m  \u001b[2m<built-in>\u001b[0m\n",
            "\n",
            "\n",
            "Total elapsed time: 55.93117629000005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Summary ===\")\n",
        "for name, t in timing_callback.tool_timings:\n",
        "    print(f\"{name:<20} {t:.3f}s\")\n",
        "print(timing_callback.reasoning_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VA-PuPsAetmF",
        "outputId": "40238536-aa8b-4ebd-e37a-1edf989fcfaa"
      },
      "id": "VA-PuPsAetmF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Summary ===\n",
            "retriever            0.402s\n",
            "calculator           0.001s\n",
            "7.959821353999928\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01e9e3ea",
      "metadata": {
        "id": "01e9e3ea"
      },
      "source": [
        "## 5. Benchmark Runner\n",
        "\n",
        "Run these 3 standardized queries. Produce JSON then prose answers with citations. These are the standardized queries.\n",
        "\n",
        "*   Gross Margin Trend (or NIM if Bank)\n",
        "    *   Query: \"Report the Gross Margin (or Net Interest Margin, if a bank) over the last 5 quarters, with values.\"\n",
        "    *   Expected Output: A quarterly table of Gross Margin % (or NIM % if bank).\n",
        "\n",
        "*   Operating Expenses (Opex) YoY for 3 Years\n",
        "    *   Query: \"Show Operating Expenses for the last 3 fiscal years, year-on-year comparison.\"\n",
        "    *   Expected Output: A 3-year Opex table (absolute numbers and % change).\n",
        "\n",
        "*   Operating Efficiency Ratio\n",
        "    *   Query: \"Calculate the Operating Efficiency Ratio (Opex ÷ Operating Income) for the last 3 fiscal years, showing the working.\"\n",
        "    *   Expected Output: Table with Opex, Operating Income, and calculated ratio for 3 years.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7bddc40",
      "metadata": {
        "id": "e7bddc40"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement benchmark runner\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "683ebeda",
      "metadata": {
        "id": "683ebeda"
      },
      "source": [
        "## 6. Instrumentation\n",
        "\n",
        "Log timings: T_ingest, T_retrieve, T_rerank, T_reason, T_generate, T_total. Log tokens, cache hits, tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5425de5",
      "metadata": {
        "id": "d5425de5"
      },
      "outputs": [],
      "source": [
        "# Example instrumentation schema\n",
        "import pandas as pd\n",
        "logs = pd.DataFrame(columns=['Query','T_ingest','T_retrieve','T_rerank','T_reason','T_generate','T_total','Tokens','CacheHits','Tools'])\n",
        "logs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8c01bf4",
      "metadata": {
        "id": "e8c01bf4"
      },
      "source": [
        "## 7. Optimizations\n",
        "\n",
        "**Required Optimizations**\n",
        "\n",
        "Each team must implement at least:\n",
        "*   2 retrieval optimizations (e.g., hybrid BM25+vector, smaller embeddings, dynamic k).\n",
        "*   1 caching optimization (query cache or ratio cache).\n",
        "*   1 agentic optimization (plan pruning, parallel sub-queries).\n",
        "*   1 system optimization (async I/O, batch embedding, memory-mapped vectors)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "783f0e2e",
      "metadata": {
        "id": "783f0e2e"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement optimizations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a91ce833",
      "metadata": {
        "id": "a91ce833"
      },
      "source": [
        "## 8. Results & Plots\n",
        "\n",
        "Show baseline vs optimized. Include latency plots (p50/p95) and accuracy tables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d96550f3",
      "metadata": {
        "id": "d96550f3"
      },
      "outputs": [],
      "source": [
        "# TODO: Generate plots with matplotlib\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}